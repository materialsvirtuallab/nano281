\documentclass[aspectratio=169]{beamer}
\usepackage[utf8]{inputenc}
\usepackage{hyperref}
\usepackage{amsmath,amsfonts,amsthm,bm}
\usepackage{color}
\usepackage{minted}
\usepackage{graphicx} % Allows including images
\usepackage{booktabs} % Allows the use of \toprule, \midrule and \bottomrule in tables
\usepackage{tikz}
\usepackage[version=3]{mhchem}
\usepackage{pgfplots}
\pgfplotsset{compat=1.16} 
\setminted{fontsize=\scriptsize}

\hypersetup{
    colorlinks=true,
    linkcolor=red,
    filecolor=magenta,      
    urlcolor=red,
}

\DeclareMathOperator*{\argmax}{argmax}
\DeclareMathOperator*{\argmin}{argmin}
\let \vec \mathbf

\newcommand{\classname}{NANOx81}
\newcommand{\classyear}{Fall 2023}
\mode<presentation> {
    \usetheme{CambridgeUS}
    \setbeamertemplate{footline}[text line]{%
      \parbox{\linewidth}{\vspace*{-8pt}\classname\hfill\classyear\hfill\insertpagenumber}}

    %\setbeamertemplate{footline}[page number]
    \setbeamertemplate{navigation symbols}{}
}


\title[Generalized Additive Models and Trees]{Generalized Additive Models and Trees}

\author{Shyue Ping Ong}
\institute[UCSD]{University of California, San Diego\\
\medskip
}
\date{\classyear} % Date, can be changed to a custom date

\begin{document}


\begin{frame}
    \titlepage % Print the title page as the first slide
\end{frame}


\begin{frame}{Overview}
    \tableofcontents
\end{frame}


\section{Preliminaries}

\begin{frame}{Preliminaries}
    \begin{itemize}
        \item We have covered two broad categories of methods for regression - the highly rigid linear methods and the very flexible local methods such as kNN. 
        \item There exist an entire spectrum of methods that assuming some structured form for the unknown regression function in between these two extremes.
    \end{itemize}
\end{frame}

\section{Generalized Additive Models}

\begin{frame}{Generalized Additive Models}
    \begin{itemize}
        \item A generalized additive model has the form:
        \begin{equation*}
            E[Y|X_1, X_2, ..., X_p] = \alpha + \sum_{j=1}^p f_j(X_j)
        \end{equation*}
        \item If $f_j$ are expanded in terms of basis functions, this reduces to a least squares fit. 
        \item For generalized additive models, we fit each function using a scatterplot smoother, e.g., cubic spline or kernel smoother.
        \item Penalized residual sum of squares is given as:
        \begin{equation*}
            PRSS = \sum_{i=1}^N \left( y_i - \alpha - \sum_{j=1}^p f_j(X_j) \right)^2 + \sum_{j=1}^p \int f_j''(t_j)^2 dt_j
        \end{equation*}
        \item First term is our standard sum squared error, and the right term is penalizes discontinuities (recall section on smoothing splines).
    \end{itemize}
\end{frame}


\begin{frame}{Fitting generalized additive models}
    \begin{itemize}
        \item Each function $f_j$ is a cubic spline of component $X_j$.
        \item To obtain unique solution, we impose a further convention that the functions average to zero over the data, i.e., $\sum_{i=1}^N f_j(x_{ij}) = 0 \forall j$
        \item Backfitting algorithm:
        \begin{enumerate}
            \item Initialize $\hat{\alpha} = \frac{1}{N} \sum_{i=1}^N y_i, \hat{f_j} = 0$.
            \item Cycle through $1, 2, ... p, 1, 2, ..., p$
            \begin{eqnarray*}
                \hat{f_j} & \longleftarrow & S_j\left[ \{y_i - \hat{\alpha} - \sum_{k\neq j} \hat{f_k}(x_{ik})\}_1^N \right]\\
                \hat{f_j} &\longleftarrow & \hat{f_j} - \frac{1}{N} \sum_{i=1}^N \hat{f_j}(x_{ij})
            \end{eqnarray*}
        \end{enumerate}
        \item Conceptually, fitting a cubic smoothing spline $S_j$ to the residual $y_i - \hat{\alpha} - \sum_{k\neq j} \hat{f_k}(x_{ik})$ for each $f_j$, and iterate until all $\hat{f_j}$s stabilize.  
    \end{itemize}
\end{frame}


\begin{frame}{Extensions of Generalized Additive Models}
    \begin{itemize}
        \item Note that we are not limited to cubic splines. E.g, local polynomial and kernel methods, linear regression, and surface smoothers etc. can be used with the appropriate choice of smoother $S_j$.
        \item GAMs can be used for classification as well, using the logit \textit{link} function. For example, for binary classification:
        \begin{equation*}
            \log{\frac{P(Y=1|X)}{P(Y=0|X)}} = \log{\frac{P(Y=1|X)}{1-P(Y=1|X)}} =  \alpha + \sum_{j=1}^p f_j(X_j)
        \end{equation*}
        Very commonly used in medical research: outcomes encoded as 0 or 1 (e.g., death/relapse of disease).
    \end{itemize}
\end{frame}


\section{Trees}

\begin{frame}
\frametitle{Tree-based methods}
\begin{columns}
\column{0.6\textwidth}
\begin{itemize}
        \item Partition feature space into regions (e.g., rectangles for 2 features case), and simple model (e.g., constant) fitted into each rectangle.
        \item Classification And Regression Trees (CART)
        \begin{equation*}
            \hat{f}(X) = \sum_{m} c_m I\{(X_1,X_2) \in R_m\}
        \end{equation*}
        \item Main question: How to decide on partitions/topology?
\end{itemize}
\column{0.4\textwidth}
\begin{figure}
            \centering
            \includegraphics[width=\textwidth]{figures/cart.pdf}
        \end{figure}
\end{columns}
\end{frame}


\begin{frame}{Regression tree fitting}
    \begin{itemize}
        \item For CART, it is clear that each region should just be given by the average of the observations $y_i$ in that region to minimize sum of squares.
        \item Best partition is usually not computationally tractable.
        \item Greedy algorithm: Start with all data, choose splitting variable $X_j$ and split point $s$ such that:
        \begin{equation*}
            \min_{X_j, s} \left[ \sum_{x_i \in R_1(X_j, s)} (y_i - c_1)^2 + \sum_{x_i \in R_2(X_j, s)} (y_i - c_2)^2 \right]
        \end{equation*}
        \item For each $X_j$, splitting point $s$ can be found quickly via scanning of the variables.
        \item This process is repeated for each region to grow the tree.
        \item Choice of tree size determines complexity of model - too large a tree results in overfitting, too small results in underfitting.
    \end{itemize}
\end{frame}


\begin{frame}{Cost-Complexity Tree Pruning}
    \begin{itemize}
        \item Generate the tree until a minimum node size is achieved. 
        \item Note: perfect performance on training data can always be obtained with an arbitrarily large tree, e.g., when the final `leaf' nodes each contain only one training observation.
        \item Number of samples in a node is therefore an indicator of tree complexity.
        \item Let subtree $T \subset T_0$ be any tree that can be obtained by pruning $T_0$.
        \item Cost-complexity criterion:
        \begin{equation*}
            C_{\alpha}(T) = \sum_{m=1}^{|T|} \sum_{x_i \in R_m} (y_i - \hat{c_m})^2 + \alpha |T|
        \end{equation*}
        \item Find the subtree $T_\alpha$ that minimizes $C_{\alpha}(T)$. $\alpha$ controls complexity. Large $\alpha$ results in smaller tree.
        \item Weakest link pruning: successively collapse each node that produces the smallest increase in $\sum_{x_i \in R_m} (y_i - \hat{c_m})^2$.
    \end{itemize}
\end{frame}


\begin{frame}{Classification Trees}
    \begin{itemize}
        \item Instead of squared error, we need to use alternative \textit{node impurity} measures: 
        \begin{description}
        \item[Misclassification error] $1/N_m \sum_{i \in R_m} I(y_i \neq k(m)) = 1-\hat{p_{mk(m)}}$
        \item[Gini index] $\sum_{k \neq k'} \hat{p_{mk}}\hat{p_{mk'}}$
        \item[Cross-entropy] $-\sum_{k =1}^K \hat{p_{mk}}\log{\hat{p_{mk}}}$
        \end{description}
        \begin{figure}
            \centering
            \includegraphics[width=0.5\textwidth]{figures/node-impurity-measures.pdf}
        \end{figure}
    \end{itemize}
\end{frame}


\begin{frame}{Miscellaneous Issues with Trees}
    \begin{itemize}
        \item Trees can be highly interpretable.
        \item Instability: small data changes can lead to very different splits.
        \item Lack of smoothness
        \item For some categorical problems, a misclassification in one category is more serious than another, e.g., it is better to have a false positive for a disease than a false negative. This can be handled by weighting the loss functions appropriately.
    \end{itemize}
\end{frame}


\begin{frame}{Example: Metal-insulator classification}
    \begin{itemize}
        \item A similar problem is in Lab 2.
        \item We will only select a smaller subset of elemental properties to construct our decision tree with. Namely, \textit{'AtomicRadius', 'AtomicWeight', 'Column', 'Electronegativity', 'Row'}. These properties are available for most elements and we avoid obviously correlated features, e.g., AtomicRadius and AtomicVolume.
    \end{itemize}
\end{frame}


\begin{frame}[fragile]{Decision Tree Regressor and Classifier in scikit-learn}
\inputminted{python}{example_sklearn_decision_tree.py}
\end{frame}


\begin{frame}{Classification accuracy}
    \begin{figure}
        \centering
        \includegraphics[width=0.6\textwidth]{figures/accuracy_metal_insulator.png}
    \end{figure}
    \begin{itemize}
        \item Quite clearly, we cannot do much better than a $\sim 82\%$ accuracy (test misclassification rate of about 18\%) with a tree-depth of around 15.
        \item Also, the training and test errors diverge significantly after a depth of around 8, which indicates overfitting.
    \end{itemize}
\end{frame}


\begin{frame}{Interpreting the tree}
    \begin{itemize}
        \item A 8-deep tree is not very easy to read. Here, we will use cost-complexity pruning with a parameter $\alpha = 0.01$ to prune the tree. The resulting tree has an accuracy of around 74\%. Let's see how the decision is being made at the first few levels.
        \begin{figure}
            \centering
            \includegraphics[width=0.6\textwidth]{figures/metal_insulator_tree.png}
        \end{figure}
    \end{itemize}
\end{frame}


\begin{frame}[fragile]{Interpreting the tree, contd.}
\begin{columns}
\column{0.5\textwidth}
\begin{figure}
    \centering
    \includegraphics[width=0.8\textwidth]{figures/pt_electronegativity.pdf}
\end{figure}
\scriptsize{
\begin{itemize}
    \item Compounds with mean $\chi \leq 2.03$ are mostly classified as metals. 
    \item Compounds with mean $\chi > 2.03$ are classified as insulators, i.e., mostly ionic compounds containing chalcogenides and halides with high $\chi$.
\end{itemize}}
\column{0.5\textwidth}
\begin{tiny}
\begin{verbatim}
|--- ElectronegativityMean <= 2.03
|   |--- ColumnMin <= 2.50
|   |   |--- ElectronegativityMax <= 5.09
|   |   |   |--- class: 0
|   |   |--- ElectronegativityMax >  5.09
|   |   |   |--- class: 1
|   |--- ColumnMin >  2.50
|   |   |--- ColumnMax <= 44.50
|   |   |   |--- class: 0
|   |   |--- ColumnMax >  44.50
|   |   |   |--- class: 0
|--- ElectronegativityMean >  2.03
|   |--- AtomicRadiusMean <= 0.98
|   |   |--- AtomicWeightMean <= 22.98
|   |   |   |--- class: 1
|   |   |--- AtomicWeightMean >  22.98
|   |   |   |--- class: 1
|   |--- AtomicRadiusMean >  0.98
|   |   |--- ColumnMax <= 31.00
|   |   |   |--- class: 0
|   |   |--- ColumnMax >  31.00
|   |   |   |--- class: 1
\end{verbatim}
\end{tiny}
\end{columns}
\end{frame}


\begin{frame}
\frametitle{Feature importance}
\begin{columns}
\column{0.5\textwidth}
\begin{itemize}
\item Another way of interpreting trees is using the feature importance.
\item The importance of a feature is the (normalized) total reduction of the criterion brought by that feature.
\end{itemize}
\column{0.5\textwidth}
\begin{figure}
    \centering
    \includegraphics[width=\textwidth]{figures/feature-importance.png}
\end{figure}
\end{columns}
\end{frame}


\begin{frame}{Receiver Operating Characteristic (ROC) Curve}
\begin{columns}
\column{0.6\textwidth}
\begin{eqnarray*}
TPR & = & \frac{TP}{P} = \frac{TP}{TP + FN}\\
FPR & = & \frac{FP}{N} = \frac{FP}{TN + FP}
\end{eqnarray*}
\begin{itemize}
    \item Plot of the TPR (\textit{sensitivity}) vs FPR (1-\textit{selectivity}).
    \item $y=x$ line denotes random guessing (TPR = FPR).
    \item The greater the area under curve (AUC), better the performance.
\end{itemize}
\column{0.4\textwidth}
    \begin{figure}
    \centering
    \includegraphics[width=\textwidth]{figures/ROC.png}
    \end{figure}
\end{columns}
\end{frame}


\begin{frame}{Multivariate Adaptive Regression Splines (MARS)}
    \begin{itemize}
        \item Essentially a modification of CART to use step-wise linear regression.
        \item MARS uses piece-wise linear basis functions:
        \begin{equation*}
            (x - t)_+ = \left\{
  \begin{array}{lr}
    x - t &, x > t\\
    0 &, otherwise
  \end{array}
\right.
        \end{equation*}
        \begin{equation*}
            (t - x)_+ = \left\{
  \begin{array}{lr}
    t - x &, x < t\\
    0 &, otherwise
  \end{array}
\right.
        \end{equation*}
        \item Implementation available in the \href{https://contrib.scikit-learn.org/py-earth/index.html}{py-earth} package.
    \end{itemize}
    \begin{figure}
    \centering
    \includegraphics[width=0.25\textwidth]{figures/MARS.pdf}
\end{figure}
\end{frame}


\section{Ensemble learning}


\begin{frame}{Ensemble learning}
    \begin{itemize}
        \item So far, we have covered the basics of using a single model (linear, kernel, tree) to perform an ML prediction.
        \item In \textit{ensemble learning}, we use multiple models and average the results to improve prediction performance.
        \item Advantage: lower variance and in many cases, dramatically improved prediction performance.
        \item Disadvantage: some of the interpretability is lost in the process.
        \item Here, we will cover two of the most popular ensemble learning approaches - \textit{boosting} and \textit{bagging}.
        \item While ensemble learning can be applied to any of the previous ML methods, we will focus here on their application to decision trees.
    \end{itemize}
\end{frame}


\subsection{Boosting}


\begin{frame}{Boosting}
    \begin{itemize}
        \item One of the most successful ML approaches in the past few decades.
        \item Concept: combine many "weak" learners in a "committee".
        \item Can be used for either classification or regression.
        \item Weak classifier: One whose error rate is slightly better than random guessing.
        \item Apply weak classifier to repeatedly modified versions of data to produce a sequence of weak classifiers.
        \item Predictions from sequence are combined using weighted majority vote:
        \begin{equation*}
            G(x) = sign\left( \sum_{m=1}^M \alpha_mG_m(x)\right)
        \end{equation*}
        \item Weights $\alpha_m$ are computed by boosting algorithm and is the contribution of each weak learner $G_m(x)$.
        \item While $G(x)$ can be any classifier, we will focus here on using decision trees as the base classifier.
    \end{itemize}
\end{frame}


\begin{frame}{AdaBoost.M1 Algorithm (Classification)}
    \label{frame:adaboost}
    \begin{enumerate}
        \item Initialize observation weights as $w_i = 1/N$.
        \item For $m=1$ to $M$:
        \begin{enumerate}
            \item Fit classifier $G_m(x)$ to training data using weights $w_i$.
            \item Compute $err_m = \frac{\sum_{i=1}^{N} w_i I(y_i \neq G_m(x_i))}{\sum_{i=1}^{N} w_i}$
            \item Compute $\alpha_m = \log{\frac{1-err_m}{err_m}}$.
            \item Set $w_i = w_i\exp[\alpha_mI(y_i \neq G_m(x_i))]$, $i = 1, 2, ...N$. Conceptually, increase weights in step $m$ for observations that are misclassified in step $m-1$.
        \end{enumerate}
        \item Output $G(x) = sign\left( \sum_{m=1}^M \alpha_mG_m(x)\right)$
    \end{enumerate}
    \begin{figure}
        \centering
        \includegraphics[width=0.45\textwidth]{figures/adaboostalgo.pdf}
    \end{figure}
\end{frame}


\begin{frame}[fragile]{AdaBoost in scikit-learn}
\inputminted{python}{example_sklearn_adaboost.py}
\end{frame}


\begin{frame}
\frametitle{Gradient Boosting}
\begin{columns}
\column{0.3\textwidth}
\begin{figure}
    \centering
    \includegraphics[width=\textwidth]{figures/gradientboostingalgo.png}
    \caption{Source: \href{https://medium.com/mlreview/gradient-boosting-from-scratch-1e317ae4587d}{Gradient Boosting from Scratch}}
\end{figure}
\column{0.7\textwidth}
    \begin{itemize}
        \item We can think of the algorithm in Slide \ref{frame:adaboost} as essentially a forward stage-wise fit of an additive model $f(x) = \sum_{m=1}^M \alpha_m G_m(x)$ (refer to \cite{hastieElementsStatisticalLearning2016} for details).
        \item Greedy approach in that it seeks to maximally reduce the loss at each step, i.e., steepest descent, by adjusting the weights iteratively.
        \item In contrast, \textit{gradient boosting} attempts to fit a new learner to the residuals of the errors from the previous step. 
    \end{itemize}
\end{columns}
\end{frame}


\begin{frame}[fragile]{Gradient Boosting in Scikit-Learn}
\inputminted{python}{example_sklearn_gradient_boosting.py}
\end{frame}


\subsection{Loss functions and robustness}


\begin{frame}{Loss functions for regression}
    \begin{itemize}
        \item We have thus far focused on the squared error loss $L(y, f(x)) = (y - f(x))^2$
        \item Another common loss function is the absolute error $L(y, f(x)) = |y - f(x)|$
        \item MSE penalizes outliers with large observed residuals severely, and hence is less robust in data with long-tailed distributions.
        \item MAE is more robust against outliers.
        \item Other criteria include the Huber loss:
        \begin{equation*}
            L(y, f(x)) = \left\{ \begin{array}{cc}
                (y - f(x))^2 & |y-f(x)| \leq \delta \\
                2\delta (y-f(x) - \delta^2 & otherwise 
            \end{array}\right.
        \end{equation*}
    \end{itemize}
\end{frame}


\begin{frame}{Loss functions for binary classification}
    \begin{itemize}
        \item Consider a simple binary classification with two levels (-1, 1). The decision boundary is at 0.
        \item Using the square error does not make sense, since we only care about whether it is $>0$ or $<0$.
        \item \textit{Margin} $yf(x)$ is positive when prediction and actual value is in the same class, and negative if they are in opposite classes.
        \item Need a loss that penalizes negative values much more than positive values for margins, i.e., monotone decreasing function.
        \item Exponential loss: $L(y, f(x)) = e^{-yf(x)}$
        \item Binomial/multinomial loss (can be used for K-classes):
        \begin{equation*}
            L(y, p(x)) = -\sum_{k=1}^K I(y=G_k)f_k(x) + \log \left(\sum_{l=1}^K e^{f_l(x)}\right)
        \end{equation*}
    \end{itemize}
\end{frame}


\begin{frame}{Loss functions for binary classification}
\begin{figure}
    \centering
    \includegraphics[width=0.4\textwidth]{figures/lossfunctions.pdf}
    \caption{Loss functions for binary classification. Response: $y = \pm1$. X-axis is the margin $y \cdot f$. Misclassification : $I(\mathrm{sign}(f) \neq y)$; exponential: $e^{-yf}$; binomial deviance: $\log(1 + e^{-2yf})$; squared error: $(y - f)^2$ ; and support vector: $(1 - yf)_+$. Source: \cite{hastieElementsStatisticalLearning2016}}
\end{figure}
\end{frame}


\subsection{Bagging and Random Forests}

\begin{frame}{Random Forests}
    \begin{itemize}
        \item Bagging: average many noisy, unbiased models to reduce variance.
        \item Random forest: Grow $B$ trees at random and average the results. Classification: majority vote (mode), regression: mean.
        \item Tree growing:
        \begin{enumerate}
            \item At each branch, select $m$ variables at random from $p$ variables.
            \item Determine best split among the $m$.
            \item Split node into two daughter nodes.
            \item Repeat until minimum node size is reached.
        \end{enumerate}
        
    \end{itemize}
\end{frame}


\begin{frame}{Random Forest Algorithm}
\begin{figure}
    \centering
    \includegraphics[width=0.8\textwidth]{figures/randomforestalgo.pdf}
\end{figure}
\end{frame}



\begin{frame}{Example: Identification of Local Environments from K-edge XANES}
\begin{figure}
    \centering
    \includegraphics[width=0.4\textwidth]{figures/randomforestxanes.pdf}
    \caption{Workflow for classification of K-edge XANES spectra into one of 25 coordination environments.\cite{zhengRandomForestModels2019}}
\end{figure}
\end{frame}


\begin{frame}{Example: Identification of Local Environments from K-edge XANES}
\begin{figure}
    \centering
    \includegraphics[width=0.4\textwidth]{figures/coord_env_class_acc_radar.pdf}
    \caption{Comparison of different ML methods for K-edge XANAES classification.\cite{zhengRandomForestModels2019}}
\end{figure}
\end{frame}


\begin{frame}{Bibliography}
    \bibliographystyle{unsrt}
    \bibliography{refs}
\end{frame}




\begin{frame}
    \Huge{\centerline{The End}}
\end{frame}

\end{document}

