\documentclass[aspectratio=169]{beamer}
\usepackage[utf8]{inputenc}
\usepackage{hyperref}
\usepackage{amsmath,amsfonts,amsthm,bm}
\usepackage{color}
\usepackage{minted}
\usepackage{graphicx} % Allows including images
\usepackage{booktabs} % Allows the use of \toprule, \midrule and \bottomrule in tables
\usepackage{tikz}
\usepackage[version=3]{mhchem}
\usepackage{pgfplots}
\pgfplotsset{compat=1.16}
\setminted{fontsize=\scriptsize}
\newcommand{\classname}{NANOx81R}

\hypersetup{
    colorlinks=true,
    linkcolor=red,
    filecolor=magenta,
    urlcolor=red,
}

\DeclareMathOperator*{\argmax}{argmax}
\DeclareMathOperator*{\argmin}{argmin}
\let \vec \mathbf


\usetheme{metropolis}

\setbeamerfont{frametitle}{size=\Large,series=\bfseries}
\metroset{block=fill}


\title[Linear Methods]{Linear Methods}

\author{Shyue Ping Ong}
\institute[UCSD]{Aiiso Yufeng Li Family Department of Chemical and Nano Engineering\\
University of California, San Diego\\\url{http://materialsvirtuallab.org}}
\date{}

\begin{document}


\begin{frame}
    \titlepage % Print the title page as the first slide
\end{frame}


\begin{frame}{Overview}
    \tableofcontents
\end{frame}


\section{Preliminaries}

\begin{frame}{Preliminaries}
    \begin{itemize}
        \item We will go very deep into linear models.
        \item Most of you probably have seen linear models in some form, but we will start from scratch to further illustrate key concepts such as bias and variance.
        \item Using linear examples, we will discuss the basic machine learning concepts of model selection, cross-validation, and loss functions.
    \end{itemize}
\end{frame}

\begin{frame}{Notation}
    \begin{itemize}
        \item Capital letters, e.g., $X$ denote variables.
        \item Lower-case letters e.g., $x$, denote observations.
        \item Dummy index $j$ denotes different variables, e.g., $X_j$
        \item Dummy index $i$ denotes different observations, e.g., $x_i$
        \item Bolded variables are vector/matrices, e.g., $\mathbf{y}$, $\mathbf{X}$
    \end{itemize}
\end{frame}

\section{Linear regression}

\begin{frame}{Simplest possible model between target and feature}
    \begin{equation*}
        Y=f(X_1,X_2,...,X_p)= \beta_0 + \sum_{j=1}^p \beta_j X_j
    \end{equation*}
    $X_j$ can be:
    \begin{itemize}
        \item Quantitative inputs
        \item Transformations of quantitative inputs, e.g., log, exp, powers, etc.
        Basis expansions, e.g., $X_2 = X_1^2$, $X_3 = X_1^3$
        \item Interactions between variables, e.g., $X_1X_2$
        \item Encoding of levels of inputs
    \end{itemize}
\end{frame}


\begin{frame}{Supervised learning}
    \begin{itemize}
        \item Given a set of paired observations $\{x_{ij}, y_i\}$, what are the model parameters (in this case, the coefficients $\beta_j$) that are ``optimal''?
        \item ``Optimal'' is typically defined as minimization of some \textbf{loss function} (also known as \textbf{cost function}) that measures the error of the model.
    \end{itemize}
\end{frame}


\begin{frame}{Least squares regression}
    Consider the simple case of
    \begin{equation*}
        Y = \beta_0 + \beta_1 X_1
    \end{equation*}
    In least squares regression, the loss function is defined as the sum squared error given the $N$ observations:
    \begin{eqnarray*}
        L(Y, \hat{f}(X)) & = & \sum_{i=1}^N (y_i - f(x_i))^2 \\
        & = & \sum_{i=1}^N (y_i - \beta_0 - \beta_1 x_{i1})^2
    \end{eqnarray*}
    \end{frame}

    \begin{frame}{What are the optimal parameters $\beta_0$ and $\beta_1$?}
    \begin{eqnarray*}
        \frac{\partial L}{\partial\beta_0} & = & \sum_{i=1}^N 2 (y_i - \beta_0 - \beta_1 x_{i1})(-1) = 0\\
        \implies & & \sum_{i=1}^N y_i = N \beta_0 + \sum_{i=1}^N \beta_1 x_{i1} \\
        \implies & & \beta_0 = \bar{y} - \beta_1 \bar{x_{1}} \\
        \frac{\partial L}{\partial\beta_1} & = & \sum_{i=1}^N 2 (y_i - \beta_0 - \beta_1 x_{i1}) (-x_{i1}) = 0\\
        \implies & & \beta_1 = \frac{\sum_{i=1}^N x_{i1} y_i - N \bar{x_1}\bar{y}}{\sum_{i=1}^N x_{i1}^2 - N \bar{x_1}^2}\\
    \end{eqnarray*}
    \end{frame}

    \begin{frame}{Reformulating the general multiple linear regression as a vector equation...}
    Considering $N$ observations of
    \begin{equation*}
        y_i = \beta_0 + \beta_1 x_{i1} + + \beta_2 x_{i2} + ... + \beta_p x_{ip}
    \end{equation*}
    Let
    \begin{equation*}
        \vec{y} = \begin{pmatrix}y_1\\y_2\\...\\y_n\end{pmatrix}, \bm{\beta} = \begin{pmatrix}\beta_0\\\beta_1\\...\\\beta_p\end{pmatrix}, \vec{X} = \begin{pmatrix}1 & x_{11} & x_{12} & ... & x_{1p}\\
        1 & x_{21} & x_{22} & ... & x_{2p}\\
        \vdots & & & & \\
        1 & x_{N1} & x_{N2} & ... & x_{Np}\end{pmatrix},
    \end{equation*}
    So,
    \begin{equation*}
        \vec{y} = \vec{X}\bm{\beta}
    \end{equation*}
    Note that $\vec{y}$ is a $N \times 1$ vector,
    $\bm{\beta}$ is a $(p+1) \times 1$ vector, and $\vec{X}$ is a  $N \times (p+1)$ matrix.
\end{frame}


\begin{frame}{Reformulating the general multiple linear regression as a vector equation...}
    \begin{equation*}
        L = RSS = (\vec{y} - \vec{X}\bm{\beta})^T(\vec{y} - \vec{X}\bm{\beta})
    \end{equation*}
    Assuming (for the moment) that $\vec{X}$ has full column rank, and hence $\vec{X}^T\vec{X}$ is positive definite, It can be shown using the same principles that the following unique solution for $\bm{\beta}$ is:
    \begin{eqnarray*}
        \hat{\bm{\beta}} &=& (\vec{X}^T \vec{X})^{-1} \vec{X}^T \vec{y} \\
        \hat{\vec{y}} & = & \vec{X} \hat{\bm{\beta}} = \vec{X}(\vec{X}^T \vec{X})^{-1} \vec{X}^T \vec{y}
    \end{eqnarray*}
\end{frame}


\begin{frame}{Graphic representation of MLR with two dependent variables}
    \begin{figure}
        \centering
        \includegraphics[width=0.3\textwidth]{figures/fig3-1.pdf}
        \includegraphics[width=0.3\textwidth]{figures/fig3-2.pdf}
        \caption{MLR minimizes sum square of residuals. The projection $\vec{\hat{y}}$ represents the vector of the least squares predictions onto the hyperplane spanned by the input vectors $\vec{x_1}$ and $\vec{x_2}$. \cite{hastieElementsStatisticalLearning2016}.}
    \end{figure}
\end{frame}


\begin{frame}{Validity of least squares criterion}
    \begin{itemize}
        \item Observations are independently drawn at random.
        \item Variance of $\vec{y}$ is constant given by $\sigma^2$.
    \begin{equation*}
        \mathrm{var}(\hat{\bm{\beta}}) = (\vec{X}^T \vec{X})^{-1} \sigma^2
    \end{equation*}
    \item and $\sigma$ is estimated using:
    \begin{equation*}
        \sigma^2 = \frac{1}{N-p-1}\sum_{i=1}^N (y_i - \hat{y_i})^2
    \end{equation*}
    \end{itemize}
\end{frame}


\begin{frame}{Example materials data}
    \begin{columns}
    \begin{column}{0.7\textwidth}
        \begin{itemize}
            \item Target: Bulk modulus of elements (from Materials Project)
            \item Candidate features:
            \begin{itemize}
                \item Melting point (MP)
                \item Boiling point (MP)
                \item Atomic number (Z)
                \item Electronegativity ($\chi$)
                \item Atomic radius ($r$)
            \end{itemize}
            \item Question: Why these features?
            \item We will also add some transformations of these inputs, i.e., the square and square root of the electronegativity and the atomic radius.
    \end{itemize}
    \end{column}
    \begin{column}{0.3\textwidth}
        \begin{figure}
        \centering
        \includegraphics[width=\textwidth]{figures/elementdata.png}
    \end{figure}
    \end{column}
\end{columns}
\end{frame}


\begin{frame}[fragile]{Using pandas for easy data manipulation}
    \inputminted{python}{example_pandas_data_manipulation.py}
\end{frame}


\begin{frame}[fragile]{MLR in scikit-learn}
\inputminted{python}{example_sklearn_mlr.py}
\begin{itemize}
    \item Note that x should contain the features only; there is no need to add a 1 column for the intercept. By default, the parameter \texttt{fit\_intercept} in \texttt{sklearn.linear\_model.LinearRegression} is True. You can set it to False to do a MLR without intercept.
    \item Documentation: \href{https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html}{link}.
\end{itemize}

\end{frame}


\begin{frame}{Hypothesis Testing for Coefficients}
    \begin{itemize}
        \item To derive insights into a model, we often want to know which among the input parameters are the most relevant to the target.
        \item Under assumptions of the errors in $y$ follow a Gaussian distribution $N(0, \sigma^2)$, the errors in $\hat{\bm{\beta}}$ also have a Gaussian distribution $N(\beta, (\vec{X}^T \vec{X})^{-1} \sigma^2)$
        \item Hypothesis testing can be carried out for whether a particular $\beta_j$ is 0 using the following test statistic:
        \begin{equation*}
        t_j = \frac{\hat{\bm{\beta_j}}}{\sigma\sqrt{v_j}}
        \end{equation*}
        where $v_j$ is the $j$th diagonal element of $(\vec{X}^T \vec{X})^{-1}$. $t_j$ has a $t$ distribution with $N-p-1$ degrees of freedom (dof).
    \end{itemize}
\end{frame}


\begin{frame}{Hypothesis Testing for Groups of Coefficients}
    \begin{itemize}
        \item More often, we want to test groups of coefficient for significance. E.g., to the $k$ levels of a categorical variable.
        \item We will use the following $F$ statistic:
        \begin{equation*}
        F = \frac{(\mathrm{RSS}_0 - \mathrm{RSS}_1)/(p_1-p_0)}{\mathrm{RSS}_1/(N-p_1-1)}
        \end{equation*}
        where $\mathrm{RSS}_0$ is the RSS of the larger model with $p_0 + 1$ parameters and $\mathrm{RSS}_1$ is the RSS of the smaller model with $p_1 + 1$ parameters with $p_0 - p_1$ parameters set to zero. The $F$ statistic has a distribution of $F_{p_1-p_0,N-p_1-1}$.
    \end{itemize}
\end{frame}


\begin{frame}{Gauss-Markov Theorem}
    \begin{itemize}
        \item Consider the estimator $\hat{\theta}$ for a variable $\theta$.
        \begin{eqnarray*}
            \mathrm{MSE} & = & E(\hat{\theta} - \theta)^2 \\
            & = & \mathrm{var}(\hat{\theta}) + [E(\hat{\theta}) - \theta]^2
        \end{eqnarray*}
        \item The MSE can be broken down into the variance of the estimate itself and the square of the bias.
        \begin{block}{Gauss-Markov Theorem}
        The least squares estimator has the smallest variance among all linear \textit{unbiased} estimators.
        \end{block}
        \item However, there can be estimators that are biased with smaller MSE.
    \end{itemize}
\end{frame}



\section{Model selection}


\begin{frame}{Model performance}
    \begin{itemize}
        \item We will take a brief digression into model assessment and selection before continuing to other linear methods.
        \item Model performance is related to its performance on \textit{independent test data}, i.e., one cannot simply report a model's performance on training data alone.
        \item Note that this section is deliberately limited to high-level concepts that are universally applicable to many different models.
    \end{itemize}
\end{frame}


\begin{frame}{Typical measures of model performance}
    \begin{itemize}
        \item Mean squared error (MSE):
            \begin{equation*}
                L(Y, \hat{f}(X)) = \frac{1}{N}\sum_{i=1}^N (y_i - f(x_i))^2
            \end{equation*}
        \item Mean absolute error (MAE):
            \begin{equation*}
                L(Y, \hat{f}(X)) = \frac{1}{N}\sum_{i=1}^N \left| y_i - f(x_i) \right|
            \end{equation*}
        \item Test error: $L$ over independent test set.
        \item Training error: $L$ over training set.
    \end{itemize}
\end{frame}


\begin{frame}{Training and test errors with model complexity}
    \begin{itemize}
        \item Model complexity increases as the number of parameters increases (e.g., number of independent variables in MLR).
        \item Training errors \textbf{always} decrease with increasing model complexity.
        \item However, test errors do not have a monotonic relationship with model complexity. Test errors are high when model complexity is too low (underfitting) or too high (overfitting).
    \end{itemize}
    \begin{figure}
        \centering
        \includegraphics[width=0.35\textwidth]{figures/fig7-1.pdf}
    \end{figure}
\end{frame}


\begin{frame}{Under-fitting versus over-fitting}
    \begin{figure}
        \centering
        \includegraphics[width=0.75\textwidth]{figures/under_over_fitting.png}
        \caption{Source: Mathworks}
    \end{figure}
\end{frame}


\begin{frame}{Training, validation and test data}
    \begin{itemize}
        \item \textbf{Model selection}: estimating the performance of different models in order to choose the best one.
        \item \textbf{Model assessment}: having chosen a final model, estimating its prediction error (generalization error) on new data.
        \item In a data-rich situation, divide data into three parts:
        \begin{itemize}
            \item Training set: For training the model.
            \item Validation set: For estimating prediction error to select the model.
            \item Test set: For assessing the generalization error of the final model.
        \end{itemize}
        \item Typical training:validation:test splits are 50:25:25 or 80:10:10, or 90:5:5.
        \item Note that at no point in the model fitting and selection process should the test set be ``seen''.
    \end{itemize}
\end{frame}


\begin{frame}{$K$-fold cross validation (CV)}
    \begin{itemize}
        \item Simplest and most widely used approach for model validation.
        \item Data set is split into $K$ buckets (usually by random).
        \item Typical values of $K$ is 5 or 10. $K = N$ is known as ``leave-one-out'' CV.
        \begin{table}
        \begin{tabular}{|p{1.7cm}|p{1.7cm}|p{1.7cm}|p{1.7cm}|p{1.7cm}|}
            \hline
            \Large{Train} & \Large{Train} & \textcolor{red}{\Large{Validate}} & \Large{Train} & \Large{Train}\\
            \hline
        \end{tabular}
        \end{table}
        \item CV score is computed on the validate data set after training on the train data:
        \begin{equation*}
                CV(\hat{f}^{-k(i)},\alpha) = \frac{1}{N_{k(i)}}\sum_{i=1}^{N_{k(i)}} L(y_i, \hat{f}^{-k(i)}(x_i,\alpha))
        \end{equation*}
        \item assuming the $k^{th}$ data bucket has $N_{k(i)}$ data points and $\hat{f}^{-k(i)}$ refers to the model fitted with the $k^{th}$ data left out ($N-N_{k(i)}$ data in fitting).
    \end{itemize}
\end{frame}


\begin{frame}[fragile]{CV in scikit-learn}
\inputminted{python}{example_sklearn_cv.py}
\begin{itemize}
    \item Note that we have customized the \texttt{KFold} object passed to the \texttt{cross\_validate} method. The reason is that our element data is sorted by default, i.e., non-random. So we want to perform shuffling prior to doing the splits.
    \item Documentation: \href{https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.cross_validate.html?highlight=cross_validate#sklearn.model_selection.cross_validate}{link}.
\end{itemize}
\end{frame}


\begin{frame}{Characteristics of the example materials dataset}
    \begin{itemize}
        \item Before proceeding further, let us try to tease out some aspects of the dataset.
        \item Quite clearly, there are correlations between some sets of variables.
        \item In other words, the input features are \textbf{non-orthonormal} with each other.
        \begin{figure}
            \centering
            \includegraphics[width=0.35\textwidth]{figures/pairplot-materialsdata.png}
            \includegraphics[width=0.45\textwidth]{figures/paircorrelations-materialsdata.png}
        \end{figure}
    \end{itemize}
\end{frame}


\subsection{Loss functions and robustness}


\begin{frame}{Loss functions for regression}
    \begin{itemize}
        \item We have thus far focused on the squared error loss $L(y, f(x)) = (y - f(x))^2$
        \item Another common loss function is the absolute error $L(y, f(x)) = |y - f(x)|$
        \item MSE penalizes outliers with large observed residuals severely, and hence is less robust in data with long-tailed distributions.
        \item MAE is more robust against outliers.
        \item Other criteria include the Huber loss:
        \begin{equation*}
            L(y, f(x)) = \left\{ \begin{array}{cc}
                (y - f(x))^2 & |y-f(x)| \leq \delta \\
                2\delta (y-f(x) - \delta^2 & otherwise
            \end{array}\right.
        \end{equation*}
    \end{itemize}
\end{frame}



\begin{frame}[allowframebreaks]{Bibliography}
    \bibliographystyle{unsrt}
    \bibliography{refs}
\end{frame}


\begin{frame}
    \Huge{\centerline{The End}}
\end{frame}

\end{document}
